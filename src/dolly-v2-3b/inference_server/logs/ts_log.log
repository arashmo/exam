2023-12-08T01:11:00,707 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-12-08T01:11:00,707 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-12-08T01:11:00,719 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-12-08T01:11:00,719 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-12-08T01:11:01,386 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
2023-12-08T01:11:01,386 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
2023-12-08T01:11:02,397 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.9.0
TS Home: /usr/local/lib/python3.8/site-packages
Current directory: /source_dev/inference_server
Temp directory: /tmp
Metrics config path: /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 10240 M
Python executable: /usr/local/bin/python
Config file: /src/app/inference_server/torch.conf
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /model/model_dev/pytorch_model
Initial Models: mymodel=dolly-v2-12b.mar
Log dir: /source_dev/inference_server/logs
Metrics dir: /source_dev/inference_server/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /model/model_dev/pytorch_model
Model config: N/A
2023-12-08T01:11:02,397 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.9.0
TS Home: /usr/local/lib/python3.8/site-packages
Current directory: /source_dev/inference_server
Temp directory: /tmp
Metrics config path: /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 10240 M
Python executable: /usr/local/bin/python
Config file: /src/app/inference_server/torch.conf
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /model/model_dev/pytorch_model
Initial Models: mymodel=dolly-v2-12b.mar
Log dir: /source_dev/inference_server/logs
Metrics dir: /source_dev/inference_server/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /model/model_dev/pytorch_model
Model config: N/A
2023-12-08T01:11:02,505 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-12-08T01:11:02,505 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-12-08T01:11:02,734 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: dolly-v2-12b.mar
2023-12-08T01:11:02,734 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: dolly-v2-12b.mar
2023-12-08T01:19:43,911 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mymodel
2023-12-08T01:19:43,911 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mymodel
2023-12-08T01:19:43,912 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mymodel
2023-12-08T01:19:43,912 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mymodel
2023-12-08T01:19:43,912 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mymodel loaded.
2023-12-08T01:19:43,912 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mymodel loaded.
2023-12-08T01:19:43,912 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mymodel, count: 1
2023-12-08T01:19:43,912 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mymodel, count: 1
2023-12-08T01:19:44,184 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-12-08T01:19:44,184 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-12-08T01:19:44,216 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:19:44,216 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:19:44,987 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-12-08T01:19:44,987 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-12-08T01:19:44,995 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-12-08T01:19:44,995 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-12-08T01:19:45,006 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-12-08T01:19:45,006 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-12-08T01:19:45,007 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-12-08T01:19:45,007 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-12-08T01:19:45,025 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-12-08T01:19:45,025 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-12-08T01:19:48,049 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-12-08T01:19:48,049 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-12-08T01:19:48,702 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1701998388
2023-12-08T01:19:48,722 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:6.545341491699219|#Level:Host|#hostname:hiring-4,timestamp:1701998388
2023-12-08T01:19:48,740 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.163124084472656|#Level:Host|#hostname:hiring-4,timestamp:1701998388
2023-12-08T01:19:48,741 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:78.0|#Level:Host|#hostname:hiring-4,timestamp:1701998388
2023-12-08T01:19:48,742 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12525.140625|#Level:Host|#hostname:hiring-4,timestamp:1701998388
2023-12-08T01:19:48,742 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3054.6484375|#Level:Host|#hostname:hiring-4,timestamp:1701998388
2023-12-08T01:19:48,769 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:21.4|#Level:Host|#hostname:hiring-4,timestamp:1701998388
2023-12-08T01:19:56,301 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mymodel,model_version:default|#hostname:hiring-4,timestamp:1701998396
2023-12-08T01:20:03,542 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=121
2023-12-08T01:20:03,562 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T01:20:03,652 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T01:20:03,662 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]121
2023-12-08T01:20:03,665 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change null -> WORKER_STARTED
2023-12-08T01:20:03,665 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change null -> WORKER_STARTED
2023-12-08T01:20:03,694 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T01:20:03,695 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T01:20:03,710 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:20:03,710 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:20:03,778 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T01:20:03,790 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1701998403790
2023-12-08T01:20:03,790 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1701998403790
2023-12-08T01:20:03,925 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - model_name: mymodel, batchSize: 1
2023-12-08T01:20:10,850 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Your torch version is 1.13.1+cu117 which does not support torch.compile
2023-12-08T01:20:10,850 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2023-12-08T01:20:10,858 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2023-12-08T01:20:40,053 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:44332 "POST /predictions/dolly-v2-12b HTTP/1.1" 404 146
2023-12-08T01:20:40,054 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:hiring-4,timestamp:1701998440
2023-12-08T01:20:48,654 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hiring-4,timestamp:1701998448
2023-12-08T01:20:48,665 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:6.544612884521484|#Level:Host|#hostname:hiring-4,timestamp:1701998448
2023-12-08T01:20:48,675 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.16385269165039|#Level:Host|#hostname:hiring-4,timestamp:1701998448
2023-12-08T01:20:48,686 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:78.0|#Level:Host|#hostname:hiring-4,timestamp:1701998448
2023-12-08T01:20:48,695 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6735.140625|#Level:Host|#hostname:hiring-4,timestamp:1701998448
2023-12-08T01:20:48,704 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:8844.97265625|#Level:Host|#hostname:hiring-4,timestamp:1701998448
2023-12-08T01:20:48,704 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:57.7|#Level:Host|#hostname:hiring-4,timestamp:1701998448
2023-12-08T01:20:50,441 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mymodel,model_version:default|#hostname:hiring-4,timestamp:1701998450
2023-12-08T01:21:48,715 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1701998508
2023-12-08T01:21:48,717 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:6.544612884521484|#Level:Host|#hostname:hiring-4,timestamp:1701998508
2023-12-08T01:21:48,735 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.16385269165039|#Level:Host|#hostname:hiring-4,timestamp:1701998508
2023-12-08T01:21:48,745 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:78.0|#Level:Host|#hostname:hiring-4,timestamp:1701998508
2023-12-08T01:21:48,745 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:1903.91796875|#Level:Host|#hostname:hiring-4,timestamp:1701998508
2023-12-08T01:21:48,745 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:13675.8828125|#Level:Host|#hostname:hiring-4,timestamp:1701998508
2023-12-08T01:21:48,745 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:88.0|#Level:Host|#hostname:hiring-4,timestamp:1701998508
2023-12-08T01:22:03,931 [ERROR] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Number or consecutive unsuccessful inference 1
2023-12-08T01:22:03,931 [ERROR] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Number or consecutive unsuccessful inference 1
2023-12-08T01:22:03,941 [ERROR] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker did not respond in given time
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:230) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:22:03,941 [ERROR] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend worker did not respond in given time
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:230) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:22:04,304 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:22:04,304 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:22:04,305 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mymodel, error: Worker died.
2023-12-08T01:22:04,305 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mymodel, error: Worker died.
2023-12-08T01:22:04,305 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:22:04,305 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:22:04,305 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1701998524305
2023-12-08T01:22:04,305 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1701998524305
2023-12-08T01:22:04,333 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:04,333 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:04,333 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:04,333 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:04,334 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T01:22:04,334 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T01:22:05,340 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:22:05,340 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:22:08,908 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:22:08,916 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:22:08,916 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:22:08,908 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:22:08,917 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:08,917 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:08,927 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:08,927 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:08,954 [ERROR] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:175) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:348) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:22:08,954 [ERROR] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error
org.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.
	at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:175) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:348) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:22:08,955 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2023-12-08T01:22:08,955 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STOPPED
2023-12-08T01:22:08,955 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:22:08,955 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:22:08,955 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:08,955 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:08,956 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:08,956 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:08,956 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T01:22:08,956 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T01:22:08,957 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:22:08,957 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:22:08,965 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:22:08,965 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:22:09,960 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:22:09,960 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:22:22,403 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=192
2023-12-08T01:22:22,413 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T01:22:22,489 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T01:22:22,498 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]192
2023-12-08T01:22:22,499 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T01:22:22,499 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T01:22:22,499 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:22:22,499 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:22:22,508 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:22:22,508 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:22:22,527 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:22:22,527 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:22:22,527 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T01:22:22,529 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:22:22,529 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:22:22,529 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:22:22,529 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:22:22,555 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:22:22,555 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:22:22,556 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:22:22,556 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:22:22,574 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:22,574 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:22,575 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:22,575 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:22,575 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-12-08T01:22:22,575 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-12-08T01:22:22,702 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:22:22,702 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:22:22,703 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:22:22,703 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:22:24,578 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:22:24,578 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:22:31,524 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=200
2023-12-08T01:22:31,526 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T01:22:31,611 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T01:22:31,612 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]200
2023-12-08T01:22:31,612 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T01:22:31,613 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:22:31,613 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:22:31,613 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:22:31,613 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:22:31,621 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T01:22:31,627 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:22:31,627 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:22:31,628 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:22:31,628 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:22:31,628 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:22:31,628 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:22:31,629 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:22:31,629 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:22:31,629 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:22:31,629 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:22:31,629 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:31,629 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:31,644 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:31,644 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:31,644 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2023-12-08T01:22:31,644 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2023-12-08T01:22:31,746 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:22:31,746 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:22:31,751 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:22:31,751 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:22:31,772 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T01:22:31,772 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T01:22:34,659 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:22:34,659 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:22:41,680 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=208
2023-12-08T01:22:41,690 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T01:22:41,774 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T01:22:41,775 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]208
2023-12-08T01:22:41,783 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T01:22:41,784 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T01:22:41,786 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:22:41,786 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:22:41,787 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:22:41,787 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:22:41,820 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T01:22:41,824 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:22:41,824 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:22:41,832 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:22:41,832 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:22:41,833 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:22:41,833 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:22:41,864 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:22:41,864 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:22:41,864 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:22:41,864 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:22:41,864 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:41,864 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:41,864 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:41,864 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:41,865 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2023-12-08T01:22:41,865 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2023-12-08T01:22:41,996 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:22:42,002 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:22:41,996 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:22:42,002 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:22:46,866 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:22:46,866 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:22:48,486 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1701998568
2023-12-08T01:22:48,494 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:6.544589996337891|#Level:Host|#hostname:hiring-4,timestamp:1701998568
2023-12-08T01:22:48,495 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.163875579833984|#Level:Host|#hostname:hiring-4,timestamp:1701998568
2023-12-08T01:22:48,495 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:78.0|#Level:Host|#hostname:hiring-4,timestamp:1701998568
2023-12-08T01:22:48,495 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12509.5625|#Level:Host|#hostname:hiring-4,timestamp:1701998568
2023-12-08T01:22:48,495 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3070.55859375|#Level:Host|#hostname:hiring-4,timestamp:1701998568
2023-12-08T01:22:48,495 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:21.5|#Level:Host|#hostname:hiring-4,timestamp:1701998568
2023-12-08T01:22:48,682 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1088, in emit
    stream.write(msg + self.terminator)
BrokenPipeError: [Errno 32] Broken pipe
Call stack:
  File "ts/metrics/metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "/usr/local/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('208', 0)

2023-12-08T01:22:48,682 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1088, in emit
    stream.write(msg + self.terminator)
BrokenPipeError: [Errno 32] Broken pipe
Call stack:
  File "ts/metrics/metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "/usr/local/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('208', 0)

2023-12-08T01:22:53,825 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=216
2023-12-08T01:22:53,843 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T01:22:53,913 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T01:22:53,914 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]216
2023-12-08T01:22:53,914 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T01:22:53,914 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:22:53,914 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:22:53,914 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:22:53,914 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:22:53,922 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T01:22:53,923 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T01:22:53,925 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:22:53,925 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:22:53,925 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:22:53,925 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:22:53,925 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:22:53,925 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:22:53,926 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:22:53,926 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:22:53,926 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:22:53,926 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:22:53,926 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:53,926 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:22:53,926 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:53,926 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:22:53,926 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2023-12-08T01:22:53,926 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2023-12-08T01:22:54,042 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:22:54,042 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:22:54,057 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:22:54,057 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:23:01,932 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:23:01,932 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:23:08,775 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=226
2023-12-08T01:23:08,784 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T01:23:08,866 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T01:23:08,875 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]226
2023-12-08T01:23:08,875 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T01:23:08,875 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T01:23:08,879 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:23:08,879 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:23:08,879 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:23:08,879 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:23:08,883 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:23:08,883 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:23:08,883 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:23:08,883 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:23:08,883 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:23:08,883 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:23:08,884 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:23:08,884 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:23:08,884 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:23:08,884 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:23:08,884 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T01:23:08,902 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:23:08,902 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:23:08,903 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:23:08,903 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:23:08,903 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2023-12-08T01:23:08,903 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2023-12-08T01:23:08,988 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:23:08,988 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:23:08,993 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:23:08,993 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:23:09,132 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mymodel,model_version:default|#hostname:hiring-4,timestamp:1701998589
2023-12-08T01:23:21,913 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:23:21,913 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:23:28,968 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=235
2023-12-08T01:23:28,969 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T01:23:29,048 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T01:23:29,049 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]235
2023-12-08T01:23:29,057 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:23:29,057 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:23:29,057 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:23:29,057 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:23:29,058 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T01:23:29,058 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T01:23:29,059 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:23:29,059 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:23:29,077 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:23:29,077 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:23:29,077 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.ReentrantLock$Sync.lockInterruptibly(ReentrantLock.java:159) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:372) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:363) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:194) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:23:29,077 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.ReentrantLock$Sync.lockInterruptibly(ReentrantLock.java:159) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:372) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:363) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:194) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:23:29,078 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:23:29,078 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:23:29,078 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:23:29,078 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:23:29,079 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:23:29,079 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:23:29,079 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:23:29,079 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:23:29,097 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2023-12-08T01:23:29,097 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2023-12-08T01:23:29,248 [ERROR] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T01:23:29,248 [ERROR] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T01:23:29,249 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:23:29,249 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:23:29,250 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:23:29,250 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:23:50,081 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:23:50,081 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:23:57,018 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=243
2023-12-08T01:23:57,029 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T01:23:57,108 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T01:23:57,109 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]243
2023-12-08T01:23:57,117 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:23:57,117 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:23:57,118 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:23:57,118 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:23:57,119 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:23:57,119 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:23:57,119 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1701998637119
2023-12-08T01:23:57,119 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1701998637119
2023-12-08T01:23:57,109 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T01:23:57,119 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:23:57,119 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T01:23:57,120 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T01:23:57,119 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:23:57,127 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: DefaultChannelPromise@403c581a(incomplete)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:243) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:131) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:403) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:119) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:209) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:23:57,127 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: DefaultChannelPromise@403c581a(incomplete)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:243) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:131) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:403) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:119) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:209) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:23:57,128 [INFO ] W-9000-mymodel_1.0 ACCESS_LOG - /127.0.0.1:51034 "POST /predictions/mymodel HTTP/1.1" 500 240866
2023-12-08T01:23:57,137 [INFO ] W-9000-mymodel_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:hiring-4,timestamp:1701998637
2023-12-08T01:23:57,138 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.job.Job - Waiting time ns: 240788575690, Inference time ns: 240807405530
2023-12-08T01:23:57,138 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.job.Job - Waiting time ns: 240788575690, Inference time ns: 240807405530
2023-12-08T01:23:57,138 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:23:57,138 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:23:57,138 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:23:57,138 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:23:57,138 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:23:57,138 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:23:57,138 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:23:57,138 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:23:57,139 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2023-12-08T01:23:57,139 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2023-12-08T01:23:57,240 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:23:57,240 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:23:57,241 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:23:57,241 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:24:31,140 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:24:31,140 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:24:37,950 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=251
2023-12-08T01:24:37,961 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T01:24:38,047 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T01:24:38,056 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]251
2023-12-08T01:24:38,057 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T01:24:38,058 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:24:38,058 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:24:38,058 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:24:38,058 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:24:38,067 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:24:38,067 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:24:38,068 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:24:38,068 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:24:38,068 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:24:38,068 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:24:38,076 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T01:24:38,086 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T01:24:38,077 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:24:38,077 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:24:38,086 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:24:38,086 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:24:38,086 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:24:38,086 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:24:38,086 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:24:38,086 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:24:38,086 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2023-12-08T01:24:38,086 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2023-12-08T01:24:38,219 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:24:38,219 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:24:38,221 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:24:38,221 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:25:01,661 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mymodel,model_version:default|#hostname:hiring-4,timestamp:1701998701
2023-12-08T01:25:33,090 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:25:33,090 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:25:39,888 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=259
2023-12-08T01:25:39,897 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T01:25:39,985 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T01:25:39,994 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]259
2023-12-08T01:25:39,995 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:25:39,995 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T01:25:39,995 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:25:39,995 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:25:39,995 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:25:39,996 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:25:39,996 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:25:39,996 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:25:39,996 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:25:39,996 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:25:40,015 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T01:25:40,016 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T01:25:39,996 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:25:40,036 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:25:40,036 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:25:40,036 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:25:40,036 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:25:40,036 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:25:40,036 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:25:40,046 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:25:40,046 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:25:40,056 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2023-12-08T01:25:40,056 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2023-12-08T01:25:40,156 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:25:40,156 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:25:40,165 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:25:40,165 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:26:01,431 [INFO ] epollEventLoopGroup-3-6 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mymodel,model_version:default|#hostname:hiring-4,timestamp:1701998761
2023-12-08T01:27:09,049 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:27:09,049 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T01:27:15,938 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=268
2023-12-08T01:27:15,939 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T01:27:16,008 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T01:27:16,009 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]268
2023-12-08T01:27:16,018 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:27:16,018 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T01:27:16,018 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:27:16,018 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T01:27:16,027 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T01:27:16,027 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T01:27:16,028 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T01:27:16,028 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:27:16,028 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T01:27:16,028 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:27:16,028 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T01:27:16,028 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:27:16,028 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T01:27:16,029 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:27:16,029 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T01:27:16,029 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:27:16,029 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T01:27:16,037 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:27:16,037 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T01:27:16,038 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:27:16,038 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T01:27:16,129 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:27:16,129 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T01:27:16,136 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T01:27:16,136 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:39:11,473 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-12-08T15:39:11,473 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-12-08T15:39:11,494 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-12-08T15:39:11,494 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-12-08T15:39:11,837 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
2023-12-08T15:39:11,837 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
2023-12-08T15:39:12,361 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.9.0
TS Home: /usr/local/lib/python3.8/site-packages
Current directory: /source_dev/inference_server
Temp directory: /tmp
Metrics config path: /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 10240 M
Python executable: /usr/local/bin/python
Config file: /src/app/inference_server/torch.conf
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /model/model_dev/pytorch_model
Initial Models: mymodel=dolly-v2-2b.mar
Log dir: /source_dev/inference_server/logs
Metrics dir: /source_dev/inference_server/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /model/model_dev/pytorch_model
Model config: N/A
2023-12-08T15:39:12,361 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.9.0
TS Home: /usr/local/lib/python3.8/site-packages
Current directory: /source_dev/inference_server
Temp directory: /tmp
Metrics config path: /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 10240 M
Python executable: /usr/local/bin/python
Config file: /src/app/inference_server/torch.conf
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /model/model_dev/pytorch_model
Initial Models: mymodel=dolly-v2-2b.mar
Log dir: /source_dev/inference_server/logs
Metrics dir: /source_dev/inference_server/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /model/model_dev/pytorch_model
Model config: N/A
2023-12-08T15:39:12,412 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-12-08T15:39:12,412 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-12-08T15:39:12,529 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: dolly-v2-2b.mar
2023-12-08T15:39:12,529 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: dolly-v2-2b.mar
2023-12-08T15:39:12,577 [INFO ] main org.pytorch.serve.archive.model.ModelArchive - createTempDir /tmp/models/a88ae937cc4c4a09ab54d9bd5dcf46c8
2023-12-08T15:39:12,577 [INFO ] main org.pytorch.serve.archive.model.ModelArchive - createTempDir /tmp/models/a88ae937cc4c4a09ab54d9bd5dcf46c8
2023-12-08T15:39:12,577 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: dolly-v2-2b.mar
org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: dolly-v2-2b.mar
	at org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:117) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:172) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:138) ~[model-server.jar:?]
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:264) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:396) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:118) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.main(ModelServer.java:99) [model-server.jar:?]
2023-12-08T15:39:12,577 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: dolly-v2-2b.mar
org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: dolly-v2-2b.mar
	at org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:117) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:172) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:138) ~[model-server.jar:?]
	at org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:264) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:396) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:118) [model-server.jar:?]
	at org.pytorch.serve.ModelServer.main(ModelServer.java:99) [model-server.jar:?]
2023-12-08T15:39:12,637 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-12-08T15:39:12,637 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-12-08T15:39:13,022 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-12-08T15:39:13,022 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-12-08T15:39:13,024 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-12-08T15:39:13,024 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-12-08T15:39:13,033 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-12-08T15:39:13,033 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-12-08T15:39:13,042 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-12-08T15:39:13,042 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-12-08T15:39:13,043 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-12-08T15:39:13,043 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-12-08T15:39:15,690 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702049955
2023-12-08T15:39:15,701 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:11.246627807617188|#Level:Host|#hostname:hiring-4,timestamp:1702049955
2023-12-08T15:39:15,702 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.461837768554688|#Level:Host|#hostname:hiring-4,timestamp:1702049955
2023-12-08T15:39:15,702 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:62.1|#Level:Host|#hostname:hiring-4,timestamp:1702049955
2023-12-08T15:39:15,702 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12369.21875|#Level:Host|#hostname:hiring-4,timestamp:1702049955
2023-12-08T15:39:15,702 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3211.078125|#Level:Host|#hostname:hiring-4,timestamp:1702049955
2023-12-08T15:39:15,730 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:22.4|#Level:Host|#hostname:hiring-4,timestamp:1702049955
2023-12-08T15:40:15,250 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702050015
2023-12-08T15:40:15,252 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:11.246623992919922|#Level:Host|#hostname:hiring-4,timestamp:1702050015
2023-12-08T15:40:15,260 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.461841583251953|#Level:Host|#hostname:hiring-4,timestamp:1702050015
2023-12-08T15:40:15,261 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:62.1|#Level:Host|#hostname:hiring-4,timestamp:1702050015
2023-12-08T15:40:15,261 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12467.51171875|#Level:Host|#hostname:hiring-4,timestamp:1702050015
2023-12-08T15:40:15,261 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3112.59375|#Level:Host|#hostname:hiring-4,timestamp:1702050015
2023-12-08T15:40:15,261 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:21.7|#Level:Host|#hostname:hiring-4,timestamp:1702050015
2023-12-08T15:41:15,189 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702050075
2023-12-08T15:41:15,200 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:11.246623992919922|#Level:Host|#hostname:hiring-4,timestamp:1702050075
2023-12-08T15:41:15,209 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.461841583251953|#Level:Host|#hostname:hiring-4,timestamp:1702050075
2023-12-08T15:41:15,226 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:62.1|#Level:Host|#hostname:hiring-4,timestamp:1702050075
2023-12-08T15:41:15,226 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12452.015625|#Level:Host|#hostname:hiring-4,timestamp:1702050075
2023-12-08T15:41:15,226 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3128.09375|#Level:Host|#hostname:hiring-4,timestamp:1702050075
2023-12-08T15:41:15,227 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:21.8|#Level:Host|#hostname:hiring-4,timestamp:1702050075
2023-12-08T15:42:15,219 [INFO ] pool-3-thread-2 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702050135
2023-12-08T15:42:15,236 [INFO ] pool-3-thread-2 TS_METRICS - DiskAvailable.Gigabytes:11.246623992919922|#Level:Host|#hostname:hiring-4,timestamp:1702050135
2023-12-08T15:42:15,236 [INFO ] pool-3-thread-2 TS_METRICS - DiskUsage.Gigabytes:18.461841583251953|#Level:Host|#hostname:hiring-4,timestamp:1702050135
2023-12-08T15:42:15,236 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:62.1|#Level:Host|#hostname:hiring-4,timestamp:1702050135
2023-12-08T15:42:15,237 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:12451.2265625|#Level:Host|#hostname:hiring-4,timestamp:1702050135
2023-12-08T15:42:15,263 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:3128.89453125|#Level:Host|#hostname:hiring-4,timestamp:1702050135
2023-12-08T15:42:15,272 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:21.8|#Level:Host|#hostname:hiring-4,timestamp:1702050135
2023-12-08T15:42:23,363 [INFO ] epollEventLoopGroup-3-1 ACCESS_LOG - /127.0.0.1:33390 "POST /predictions/mymodel HTTP/1.1" 404 51
2023-12-08T15:42:23,373 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:hiring-4,timestamp:1702050143
2023-12-08T15:42:57,170 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /127.0.0.1:45598 "POST /predictions/mymodel HTTP/1.1" 404 0
2023-12-08T15:42:57,179 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:hiring-4,timestamp:1702050177
2023-12-08T15:43:15,225 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702050195
2023-12-08T15:43:15,234 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:11.246620178222656|#Level:Host|#hostname:hiring-4,timestamp:1702050195
2023-12-08T15:43:15,244 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.46184539794922|#Level:Host|#hostname:hiring-4,timestamp:1702050195
2023-12-08T15:43:15,244 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:62.1|#Level:Host|#hostname:hiring-4,timestamp:1702050195
2023-12-08T15:43:15,244 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12429.37109375|#Level:Host|#hostname:hiring-4,timestamp:1702050195
2023-12-08T15:43:15,245 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3150.75|#Level:Host|#hostname:hiring-4,timestamp:1702050195
2023-12-08T15:43:15,245 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:22.0|#Level:Host|#hostname:hiring-4,timestamp:1702050195
2023-12-08T15:43:30,897 [INFO ] epollEventLoopGroup-3-3 ACCESS_LOG - /127.0.0.1:47634 "POST /predictions/dolly-v2-3b HTTP/1.1" 404 1
2023-12-08T15:43:30,898 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:hiring-4,timestamp:1702050210
2023-12-08T15:43:35,183 [INFO ] epollEventLoopGroup-3-4 ACCESS_LOG - /127.0.0.1:47640 "POST /predictions/dolly-v2-3b HTTP/1.1" 404 1
2023-12-08T15:43:35,184 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:hiring-4,timestamp:1702050215
2023-12-08T15:44:15,196 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702050255
2023-12-08T15:44:15,214 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:11.24661636352539|#Level:Host|#hostname:hiring-4,timestamp:1702050255
2023-12-08T15:44:15,214 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.461849212646484|#Level:Host|#hostname:hiring-4,timestamp:1702050255
2023-12-08T15:44:15,214 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:62.1|#Level:Host|#hostname:hiring-4,timestamp:1702050255
2023-12-08T15:44:15,215 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12406.4375|#Level:Host|#hostname:hiring-4,timestamp:1702050255
2023-12-08T15:44:15,215 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3173.5|#Level:Host|#hostname:hiring-4,timestamp:1702050255
2023-12-08T15:44:15,215 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:22.1|#Level:Host|#hostname:hiring-4,timestamp:1702050255
2023-12-08T15:45:15,210 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702050315
2023-12-08T15:45:15,238 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:11.24661636352539|#Level:Host|#hostname:hiring-4,timestamp:1702050315
2023-12-08T15:45:15,247 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.461849212646484|#Level:Host|#hostname:hiring-4,timestamp:1702050315
2023-12-08T15:45:15,247 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:62.1|#Level:Host|#hostname:hiring-4,timestamp:1702050315
2023-12-08T15:45:15,247 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12414.40234375|#Level:Host|#hostname:hiring-4,timestamp:1702050315
2023-12-08T15:45:15,248 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3165.7109375|#Level:Host|#hostname:hiring-4,timestamp:1702050315
2023-12-08T15:45:15,248 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:22.1|#Level:Host|#hostname:hiring-4,timestamp:1702050315
2023-12-08T15:46:15,237 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702050375
2023-12-08T15:46:15,238 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:11.24661636352539|#Level:Host|#hostname:hiring-4,timestamp:1702050375
2023-12-08T15:46:15,247 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.461849212646484|#Level:Host|#hostname:hiring-4,timestamp:1702050375
2023-12-08T15:46:15,248 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:62.1|#Level:Host|#hostname:hiring-4,timestamp:1702050375
2023-12-08T15:46:15,248 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12394.70703125|#Level:Host|#hostname:hiring-4,timestamp:1702050375
2023-12-08T15:46:15,248 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3185.3984375|#Level:Host|#hostname:hiring-4,timestamp:1702050375
2023-12-08T15:46:15,248 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:22.2|#Level:Host|#hostname:hiring-4,timestamp:1702050375
2023-12-08T15:46:32,333 [INFO ] epollEventLoopGroup-3-5 ACCESS_LOG - /127.0.0.1:37768 "POST /predictions/mymodel HTTP/1.1" 404 0
2023-12-08T15:46:32,351 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:hiring-4,timestamp:1702050392
2023-12-08T15:47:15,236 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:hiring-4,timestamp:1702050435
2023-12-08T15:47:15,236 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:11.246612548828125|#Level:Host|#hostname:hiring-4,timestamp:1702050435
2023-12-08T15:47:15,244 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:18.46185302734375|#Level:Host|#hostname:hiring-4,timestamp:1702050435
2023-12-08T15:47:15,254 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:62.1|#Level:Host|#hostname:hiring-4,timestamp:1702050435
2023-12-08T15:47:15,254 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12322.1875|#Level:Host|#hostname:hiring-4,timestamp:1702050435
2023-12-08T15:47:15,255 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3257.71484375|#Level:Host|#hostname:hiring-4,timestamp:1702050435
2023-12-08T15:47:15,255 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:22.7|#Level:Host|#hostname:hiring-4,timestamp:1702050435
2023-12-08T15:48:42,363 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-12-08T15:48:42,363 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-12-08T15:48:42,381 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-12-08T15:48:42,381 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-12-08T15:48:42,940 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
2023-12-08T15:48:42,940 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
2023-12-08T15:48:43,713 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.9.0
TS Home: /usr/local/lib/python3.8/site-packages
Current directory: /source_dev/inference_server
Temp directory: /tmp
Metrics config path: /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 10240 M
Python executable: /usr/local/bin/python
Config file: /src/app/inference_server/torch.conf
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /model/model_dev/pytorch_model
Initial Models: mymodel=dolly-v2-3b.mar
Log dir: /source_dev/inference_server/logs
Metrics dir: /source_dev/inference_server/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /model/model_dev/pytorch_model
Model config: N/A
2023-12-08T15:48:43,713 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.9.0
TS Home: /usr/local/lib/python3.8/site-packages
Current directory: /source_dev/inference_server
Temp directory: /tmp
Metrics config path: /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 10240 M
Python executable: /usr/local/bin/python
Config file: /src/app/inference_server/torch.conf
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /model/model_dev/pytorch_model
Initial Models: mymodel=dolly-v2-3b.mar
Log dir: /source_dev/inference_server/logs
Metrics dir: /source_dev/inference_server/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /model/model_dev/pytorch_model
Model config: N/A
2023-12-08T15:48:43,792 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-12-08T15:48:43,792 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-12-08T15:48:43,921 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: dolly-v2-3b.mar
2023-12-08T15:48:43,921 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: dolly-v2-3b.mar
2023-12-08T15:57:35,849 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mymodel
2023-12-08T15:57:35,849 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mymodel
2023-12-08T15:57:35,850 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mymodel
2023-12-08T15:57:35,850 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mymodel
2023-12-08T15:57:35,851 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mymodel loaded.
2023-12-08T15:57:35,851 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mymodel loaded.
2023-12-08T15:57:35,859 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mymodel, count: 1
2023-12-08T15:57:35,859 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mymodel, count: 1
2023-12-08T15:57:36,075 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-12-08T15:57:36,075 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-12-08T15:57:36,065 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:57:36,065 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:57:36,753 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-12-08T15:57:36,753 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-12-08T15:57:36,781 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-12-08T15:57:36,781 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-12-08T15:57:36,824 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-12-08T15:57:36,824 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-12-08T15:57:36,824 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-12-08T15:57:36,824 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-12-08T15:57:36,844 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-12-08T15:57:36,844 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-12-08T15:57:39,279 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-12-08T15:57:39,279 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-12-08T15:57:39,774 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702051059
2023-12-08T15:57:39,783 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:5.950447082519531|#Level:Host|#hostname:hiring-4,timestamp:1702051059
2023-12-08T15:57:39,810 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.758018493652344|#Level:Host|#hostname:hiring-4,timestamp:1702051059
2023-12-08T15:57:39,829 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:80.0|#Level:Host|#hostname:hiring-4,timestamp:1702051059
2023-12-08T15:57:39,830 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12376.03515625|#Level:Host|#hostname:hiring-4,timestamp:1702051059
2023-12-08T15:57:39,830 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3203.66015625|#Level:Host|#hostname:hiring-4,timestamp:1702051059
2023-12-08T15:57:39,848 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:22.3|#Level:Host|#hostname:hiring-4,timestamp:1702051059
2023-12-08T15:57:45,415 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mymodel,model_version:default|#hostname:hiring-4,timestamp:1702051065
2023-12-08T15:57:55,430 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2102
2023-12-08T15:57:55,438 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T15:57:55,515 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T15:57:55,524 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2102
2023-12-08T15:57:55,534 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T15:57:55,534 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T15:57:55,537 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change null -> WORKER_STARTED
2023-12-08T15:57:55,537 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change null -> WORKER_STARTED
2023-12-08T15:57:55,585 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:57:55,585 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:57:55,652 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T15:57:55,671 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1702051075663
2023-12-08T15:57:55,671 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1702051075663
2023-12-08T15:57:55,862 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - model_name: mymodel, batchSize: 1
2023-12-08T15:58:01,822 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Your torch version is 1.13.1+cu117 which does not support torch.compile
2023-12-08T15:58:01,839 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2023-12-08T15:58:01,840 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2023-12-08T15:58:02,430 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Backend worker process died.
2023-12-08T15:58:02,430 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2023-12-08T15:58:02,431 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_service_worker.py", line 258, in <module>
2023-12-08T15:58:02,439 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     worker.run_server()
2023-12-08T15:58:02,439 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_service_worker.py", line 226, in run_server
2023-12-08T15:58:02,439 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2023-12-08T15:58:02,440 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_service_worker.py", line 189, in handle_connection
2023-12-08T15:58:02,458 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2023-12-08T15:58:02,458 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_service_worker.py", line 131, in load_model
2023-12-08T15:58:02,459 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     service = model_loader.load(
2023-12-08T15:58:02,468 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_loader.py", line 135, in load
2023-12-08T15:58:02,468 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2023-12-08T15:58:02,468 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/tmp/models/d64f117c091042179c490247244e789f/llm_handler2.py", line 28, in initialize
2023-12-08T15:58:02,469 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     self.model = AutoModelForCausalLM.from_pretrained(
2023-12-08T15:58:02,469 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
2023-12-08T15:58:02,469 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     config, kwargs = AutoConfig.from_pretrained(
2023-12-08T15:58:02,469 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
2023-12-08T15:58:02,469 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
2023-12-08T15:58:02,469 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
2023-12-08T15:58:02,469 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
2023-12-08T15:58:02,470 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
2023-12-08T15:58:02,500 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     resolved_config_file = cached_file(
2023-12-08T15:58:02,527 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/utils/hub.py", line 401, in cached_file
2023-12-08T15:58:02,527 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     raise EnvironmentError(
2023-12-08T15:58:02,527 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - OSError: /tmp/models/d64f117c091042179c490247244e789f does not appear to have a file named config.json. Checkout 'https://huggingface.co//tmp/models/d64f117c091042179c490247244e789f/None' for available files.
2023-12-08T15:58:02,584 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:58:02,584 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:58:02,593 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:58:02,593 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:58:02,594 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:219) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:58:02,594 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:219) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:58:02,715 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mymodel, error: Worker died.
2023-12-08T15:58:02,715 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mymodel, error: Worker died.
2023-12-08T15:58:02,734 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:58:02,734 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:58:02,734 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1702051082734
2023-12-08T15:58:02,734 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1702051082734
2023-12-08T15:58:02,735 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:58:02,735 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:58:02,735 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:58:02,735 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:58:02,744 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T15:58:02,744 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T15:58:02,856 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:58:02,862 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:58:02,862 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:58:02,856 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:58:03,755 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:58:03,755 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:58:10,573 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2120
2023-12-08T15:58:10,583 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T15:58:10,661 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T15:58:10,662 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2120
2023-12-08T15:58:10,672 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:58:10,672 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T15:58:10,672 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:58:10,672 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:58:10,672 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:58:10,686 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:58:10,686 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:58:10,686 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:58:10,686 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:58:10,686 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:58:10,686 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:58:10,687 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:58:10,687 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:58:10,687 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:58:10,687 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:58:10,687 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:58:10,687 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:58:10,687 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:58:10,687 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:58:10,688 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T15:58:10,688 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T15:58:10,672 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T15:58:10,690 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:58:10,690 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:58:10,876 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:58:10,876 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:58:11,688 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:58:11,688 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:58:18,455 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2128
2023-12-08T15:58:18,474 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T15:58:18,543 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T15:58:18,544 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2128
2023-12-08T15:58:18,545 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T15:58:18,545 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T15:58:18,548 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:58:18,548 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:58:18,548 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:58:18,548 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:58:18,563 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T15:58:18,565 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:58:18,565 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:58:18,566 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:58:18,566 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:58:18,566 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:58:18,566 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:58:18,567 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:58:18,567 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:58:18,567 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:58:18,567 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:58:18,567 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:58:18,567 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:58:18,604 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:58:18,604 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:58:18,605 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-12-08T15:58:18,605 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-12-08T15:58:18,691 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:58:18,691 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:58:18,691 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:58:18,691 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:58:20,605 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:58:20,605 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:58:27,314 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2136
2023-12-08T15:58:27,324 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T15:58:27,414 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T15:58:27,414 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2136
2023-12-08T15:58:27,434 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T15:58:27,434 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T15:58:27,438 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:58:27,438 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:58:27,438 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:58:27,438 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:58:27,454 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T15:58:27,462 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:58:27,462 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:58:27,462 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:58:27,462 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:58:27,463 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:58:27,463 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:58:27,463 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:58:27,463 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:58:27,463 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:58:27,463 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:58:27,463 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:58:27,463 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:58:27,464 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:58:27,464 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:58:27,464 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2023-12-08T15:58:27,464 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2023-12-08T15:58:27,589 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:58:27,589 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:58:27,624 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:58:27,624 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:58:30,464 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:58:30,464 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:58:37,093 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2144
2023-12-08T15:58:37,113 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T15:58:37,172 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T15:58:37,173 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2144
2023-12-08T15:58:37,182 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T15:58:37,182 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T15:58:37,187 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:58:37,187 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:58:37,187 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:58:37,187 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:58:37,199 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:58:37,199 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:58:37,199 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:58:37,199 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:58:37,227 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:58:37,227 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:58:37,228 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:58:37,228 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:58:37,228 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:58:37,228 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:58:37,228 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:58:37,228 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:58:37,228 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:58:37,228 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:58:37,228 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2023-12-08T15:58:37,228 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2023-12-08T15:58:37,202 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T15:58:37,232 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:58:37,232 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:58:37,323 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:58:37,323 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:58:39,662 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702051119
2023-12-08T15:58:39,662 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:5.950405120849609|#Level:Host|#hostname:hiring-4,timestamp:1702051119
2023-12-08T15:58:39,663 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.758060455322266|#Level:Host|#hostname:hiring-4,timestamp:1702051119
2023-12-08T15:58:39,663 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:80.0|#Level:Host|#hostname:hiring-4,timestamp:1702051119
2023-12-08T15:58:39,671 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12257.390625|#Level:Host|#hostname:hiring-4,timestamp:1702051119
2023-12-08T15:58:39,671 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3322.71484375|#Level:Host|#hostname:hiring-4,timestamp:1702051119
2023-12-08T15:58:39,672 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:23.1|#Level:Host|#hostname:hiring-4,timestamp:1702051119
2023-12-08T15:58:39,761 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1088, in emit
    stream.write(msg + self.terminator)
BrokenPipeError: [Errno 32] Broken pipe
Call stack:
  File "ts/metrics/metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "/usr/local/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('2144', 0)

2023-12-08T15:58:39,761 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1088, in emit
    stream.write(msg + self.terminator)
BrokenPipeError: [Errno 32] Broken pipe
Call stack:
  File "ts/metrics/metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "/usr/local/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('2144', 0)

2023-12-08T15:58:42,233 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:58:42,233 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:58:48,853 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2154
2023-12-08T15:58:48,854 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T15:58:48,932 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T15:58:48,933 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2154
2023-12-08T15:58:48,933 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T15:58:48,934 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:58:48,934 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:58:48,935 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:58:48,935 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:58:48,941 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T15:58:48,946 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T15:58:48,954 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:58:48,954 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:58:48,954 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:58:48,954 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:58:48,955 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:58:48,955 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:58:48,955 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:58:48,955 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:58:48,955 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:58:48,955 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:58:48,955 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:58:48,955 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:58:48,956 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:58:48,956 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:58:48,956 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2023-12-08T15:58:48,956 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2023-12-08T15:58:49,091 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:58:49,091 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:58:49,068 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:58:49,068 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:58:56,963 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:58:56,963 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:59:03,740 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2162
2023-12-08T15:59:03,757 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T15:59:03,822 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T15:59:03,831 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2162
2023-12-08T15:59:03,831 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:59:03,831 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:59:03,831 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:59:03,831 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:59:03,839 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:59:03,832 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T15:59:03,839 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:59:03,839 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1702051143839
2023-12-08T15:59:03,839 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1702051143839
2023-12-08T15:59:03,844 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:59:03,844 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:59:03,844 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: DefaultChannelPromise@c7f14d3(incomplete)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:243) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:131) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:403) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:119) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:209) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:59:03,844 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: DefaultChannelPromise@c7f14d3(incomplete)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:243) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:131) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:403) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:119) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:209) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:59:03,840 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T15:59:03,901 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T15:59:03,961 [INFO ] W-9000-mymodel_1.0 ACCESS_LOG - /127.0.0.1:55330 "POST /predictions/mymodel HTTP/1.1" 500 78576
2023-12-08T15:59:03,990 [INFO ] W-9000-mymodel_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:hiring-4,timestamp:1702051143
2023-12-08T15:59:04,010 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.job.Job - Waiting time ns: 78392887133, Inference time ns: 78563967090
2023-12-08T15:59:04,010 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.job.Job - Waiting time ns: 78392887133, Inference time ns: 78563967090
2023-12-08T15:59:04,011 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:59:04,011 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:59:04,011 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:59:04,011 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:59:04,011 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:59:04,011 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:59:04,011 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:59:04,011 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:59:04,011 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2023-12-08T15:59:04,011 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2023-12-08T15:59:04,118 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:59:04,118 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:59:04,119 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:59:04,119 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:59:17,019 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:59:17,019 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:59:23,849 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2170
2023-12-08T15:59:23,858 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T15:59:23,947 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T15:59:23,948 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2170
2023-12-08T15:59:23,948 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T15:59:23,948 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T15:59:23,952 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:59:23,952 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:59:23,952 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:59:23,952 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:59:23,957 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:59:23,957 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:59:23,957 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:59:23,957 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:59:23,957 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:59:23,957 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:59:23,957 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:59:23,957 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:59:23,957 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:59:23,957 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:59:23,958 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:59:23,958 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:59:23,958 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:59:23,958 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:59:23,958 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2023-12-08T15:59:23,958 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2023-12-08T15:59:24,015 [ERROR] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T15:59:24,015 [ERROR] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T15:59:24,024 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:59:24,024 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:59:24,034 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:59:24,034 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:59:44,964 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:59:44,964 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T15:59:51,879 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2178
2023-12-08T15:59:51,898 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T15:59:51,955 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T15:59:51,957 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2178
2023-12-08T15:59:51,957 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:59:51,957 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T15:59:51,957 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:59:51,957 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T15:59:51,976 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:59:51,976 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T15:59:51,976 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:59:51,976 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T15:59:51,976 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:59:51,976 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T15:59:51,977 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T15:59:51,985 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:59:51,985 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T15:59:51,986 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:59:51,986 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T15:59:51,986 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:59:51,986 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T15:59:51,986 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:59:51,986 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T15:59:51,986 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2023-12-08T15:59:51,986 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2023-12-08T15:59:51,986 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T15:59:51,987 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:59:51,987 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T15:59:52,065 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T15:59:52,065 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:00:35,380 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-12-08T16:00:35,380 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-12-08T16:00:35,399 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-12-08T16:00:35,399 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-12-08T16:00:36,032 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
2023-12-08T16:00:36,032 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
2023-12-08T16:00:36,773 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.9.0
TS Home: /usr/local/lib/python3.8/site-packages
Current directory: /source_dev/inference_server
Temp directory: /tmp
Metrics config path: /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 10240 M
Python executable: /usr/local/bin/python
Config file: /src/app/inference_server/torch.conf
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /model/model_dev/pytorch_model
Initial Models: mymodel=dolly-v2-3b.mar
Log dir: /source_dev/inference_server/logs
Metrics dir: /source_dev/inference_server/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /model/model_dev/pytorch_model
Model config: N/A
2023-12-08T16:00:36,773 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.9.0
TS Home: /usr/local/lib/python3.8/site-packages
Current directory: /source_dev/inference_server
Temp directory: /tmp
Metrics config path: /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 10240 M
Python executable: /usr/local/bin/python
Config file: /src/app/inference_server/torch.conf
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /model/model_dev/pytorch_model
Initial Models: mymodel=dolly-v2-3b.mar
Log dir: /source_dev/inference_server/logs
Metrics dir: /source_dev/inference_server/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /model/model_dev/pytorch_model
Model config: N/A
2023-12-08T16:00:36,839 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-12-08T16:00:36,839 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-12-08T16:00:36,980 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: dolly-v2-3b.mar
2023-12-08T16:00:36,980 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: dolly-v2-3b.mar
2023-12-08T16:09:30,875 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mymodel
2023-12-08T16:09:30,875 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mymodel
2023-12-08T16:09:30,894 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mymodel
2023-12-08T16:09:30,894 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mymodel
2023-12-08T16:09:30,903 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mymodel loaded.
2023-12-08T16:09:30,903 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mymodel loaded.
2023-12-08T16:09:30,903 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mymodel, count: 1
2023-12-08T16:09:30,903 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mymodel, count: 1
2023-12-08T16:09:30,981 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:09:30,989 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-12-08T16:09:30,989 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-12-08T16:09:30,981 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:09:31,671 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-12-08T16:09:31,671 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-12-08T16:09:31,681 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-12-08T16:09:31,681 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-12-08T16:09:31,700 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-12-08T16:09:31,700 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-12-08T16:09:31,702 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-12-08T16:09:31,702 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-12-08T16:09:31,711 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-12-08T16:09:31,711 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-12-08T16:09:33,480 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-12-08T16:09:33,480 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-12-08T16:09:34,058 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702051774
2023-12-08T16:09:34,085 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:5.950382232666016|#Level:Host|#hostname:hiring-4,timestamp:1702051774
2023-12-08T16:09:34,087 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.75808334350586|#Level:Host|#hostname:hiring-4,timestamp:1702051774
2023-12-08T16:09:34,095 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:80.0|#Level:Host|#hostname:hiring-4,timestamp:1702051774
2023-12-08T16:09:34,096 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12366.91796875|#Level:Host|#hostname:hiring-4,timestamp:1702051774
2023-12-08T16:09:34,105 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3213.1796875|#Level:Host|#hostname:hiring-4,timestamp:1702051774
2023-12-08T16:09:34,105 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:22.4|#Level:Host|#hostname:hiring-4,timestamp:1702051774
2023-12-08T16:09:39,736 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2244
2023-12-08T16:09:39,750 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:09:39,816 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:09:39,817 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2244
2023-12-08T16:09:39,826 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:09:39,826 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:09:39,830 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change null -> WORKER_STARTED
2023-12-08T16:09:39,830 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change null -> WORKER_STARTED
2023-12-08T16:09:39,864 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:09:39,864 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:09:39,956 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:09:39,975 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1702051779974
2023-12-08T16:09:39,975 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1702051779974
2023-12-08T16:09:40,194 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - model_name: mymodel, batchSize: 1
2023-12-08T16:09:46,958 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Your torch version is 1.13.1+cu117 which does not support torch.compile
2023-12-08T16:09:46,958 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2023-12-08T16:09:46,959 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2023-12-08T16:09:47,707 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Backend worker process died.
2023-12-08T16:09:47,708 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2023-12-08T16:09:47,708 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_service_worker.py", line 258, in <module>
2023-12-08T16:09:47,708 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     worker.run_server()
2023-12-08T16:09:47,708 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_service_worker.py", line 226, in run_server
2023-12-08T16:09:47,708 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2023-12-08T16:09:47,708 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_service_worker.py", line 189, in handle_connection
2023-12-08T16:09:47,709 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2023-12-08T16:09:47,719 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_service_worker.py", line 131, in load_model
2023-12-08T16:09:47,737 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     service = model_loader.load(
2023-12-08T16:09:47,737 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_loader.py", line 135, in load
2023-12-08T16:09:47,737 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2023-12-08T16:09:47,756 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/tmp/models/60fb9d8f556c4d1d86a2d660009e817e/llm_handler2.py", line 28, in initialize
2023-12-08T16:09:47,756 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     self.model = AutoModelForCausalLM.from_pretrained(
2023-12-08T16:09:47,756 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
2023-12-08T16:09:47,756 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     config, kwargs = AutoConfig.from_pretrained(
2023-12-08T16:09:47,756 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
2023-12-08T16:09:47,774 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
2023-12-08T16:09:47,775 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
2023-12-08T16:09:47,775 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
2023-12-08T16:09:47,775 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
2023-12-08T16:09:47,775 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     resolved_config_file = cached_file(
2023-12-08T16:09:47,776 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/utils/hub.py", line 401, in cached_file
2023-12-08T16:09:47,794 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     raise EnvironmentError(
2023-12-08T16:09:47,794 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - OSError: /tmp/models/60fb9d8f556c4d1d86a2d660009e817e does not appear to have a file named config.json. Checkout 'https://huggingface.co//tmp/models/60fb9d8f556c4d1d86a2d660009e817e/None' for available files.
2023-12-08T16:09:47,919 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:09:47,919 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:09:47,957 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:09:47,957 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:09:47,958 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:219) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:09:47,958 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:219) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:09:48,227 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mymodel, error: Worker died.
2023-12-08T16:09:48,227 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mymodel, error: Worker died.
2023-12-08T16:09:48,236 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:09:48,236 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:09:48,236 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1702051788236
2023-12-08T16:09:48,236 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1702051788236
2023-12-08T16:09:48,246 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:09:48,246 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:09:48,246 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:09:48,246 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:09:48,247 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T16:09:48,247 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T16:09:48,392 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:09:48,392 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:09:48,407 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:09:48,407 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:09:49,261 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:09:49,261 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:09:56,256 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2261
2023-12-08T16:09:56,266 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:09:56,356 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:09:56,365 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2261
2023-12-08T16:09:56,365 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:09:56,365 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:09:56,365 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:09:56,375 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:09:56,375 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:09:56,377 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:09:56,385 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:09:56,397 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:09:56,397 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:09:56,397 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:09:56,397 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:09:56,398 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:09:56,398 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:09:56,398 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:09:56,398 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:09:56,417 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:09:56,417 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:09:56,417 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:09:56,417 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:09:56,417 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:09:56,417 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:09:56,417 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T16:09:56,417 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T16:09:56,532 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:09:56,532 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:09:56,542 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:09:56,542 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:09:57,423 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:09:57,423 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:10:04,108 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2269
2023-12-08T16:10:04,112 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:10:04,176 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:10:04,177 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2269
2023-12-08T16:10:04,177 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:10:04,177 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:10:04,178 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:10:04,178 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:10:04,186 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:10:04,186 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:10:04,196 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:10:04,201 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:10:04,201 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:10:04,201 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:10:04,201 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:10:04,201 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:10:04,201 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:10:04,203 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:10:04,203 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:10:04,229 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:10:04,229 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:10:04,230 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:10:04,230 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:10:04,230 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:10:04,230 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:10:04,230 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-12-08T16:10:04,230 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-12-08T16:10:04,323 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:10:04,323 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:10:04,332 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:10:04,332 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:10:06,238 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:10:06,238 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:10:13,067 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2277
2023-12-08T16:10:13,069 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:10:13,147 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:10:13,148 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2277
2023-12-08T16:10:13,148 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:10:13,148 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:10:13,148 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:10:13,148 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:10:13,148 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:10:13,157 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:10:13,186 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:10:13,186 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:10:13,187 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:10:13,187 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:10:13,187 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:367) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:194) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:10:13,187 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:367) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:194) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:10:13,196 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:10:13,197 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:10:13,197 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:10:13,197 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:10:13,197 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:10:13,198 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:10:13,198 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:10:13,198 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:10:13,198 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:10:13,199 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2023-12-08T16:10:13,199 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2023-12-08T16:10:13,345 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:10:13,345 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:10:13,347 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:10:13,347 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:10:16,202 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:10:16,202 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:10:23,140 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2285
2023-12-08T16:10:23,150 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:10:23,236 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:10:23,238 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2285
2023-12-08T16:10:23,247 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:10:23,247 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:10:23,247 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:10:23,247 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:10:23,256 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:10:23,257 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:10:23,266 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:10:23,266 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:10:23,267 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:10:23,273 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:10:23,273 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:10:23,273 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:10:23,273 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:10:23,274 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:10:23,274 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:10:23,274 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:10:23,274 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:10:23,274 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:10:23,274 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:10:23,274 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:10:23,274 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:10:23,274 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2023-12-08T16:10:23,274 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2023-12-08T16:10:23,382 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:10:23,382 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:10:23,382 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:10:23,382 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:10:28,279 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:10:28,279 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:10:33,892 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702051833
2023-12-08T16:10:33,892 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:5.950340270996094|#Level:Host|#hostname:hiring-4,timestamp:1702051833
2023-12-08T16:10:33,893 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.75812530517578|#Level:Host|#hostname:hiring-4,timestamp:1702051833
2023-12-08T16:10:33,893 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:80.0|#Level:Host|#hostname:hiring-4,timestamp:1702051833
2023-12-08T16:10:33,901 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12274.5234375|#Level:Host|#hostname:hiring-4,timestamp:1702051833
2023-12-08T16:10:33,902 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3305.58984375|#Level:Host|#hostname:hiring-4,timestamp:1702051833
2023-12-08T16:10:33,902 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:23.0|#Level:Host|#hostname:hiring-4,timestamp:1702051833
2023-12-08T16:10:35,001 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2293
2023-12-08T16:10:35,002 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:10:35,091 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:10:35,091 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2293
2023-12-08T16:10:35,092 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:10:35,092 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:10:35,100 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:10:35,100 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:10:35,100 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:10:35,100 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:10:35,111 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:10:35,121 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:10:35,121 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:10:35,121 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:10:35,121 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:10:35,121 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:10:35,121 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:10:35,122 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:10:35,122 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:10:35,122 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:10:35,122 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:10:35,122 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:10:35,122 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:10:35,122 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:10:35,122 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:10:35,122 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2023-12-08T16:10:35,122 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2023-12-08T16:10:35,210 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:10:35,210 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:10:35,212 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:10:35,212 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:10:43,128 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:10:43,128 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:10:49,880 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2303
2023-12-08T16:10:49,881 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:10:49,967 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:10:49,968 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2303
2023-12-08T16:10:49,969 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:10:49,969 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:10:49,974 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:10:49,974 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:10:49,974 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:10:49,974 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:10:50,026 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:10:50,026 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:10:50,026 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:10:50,026 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:10:50,026 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:10:50,026 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:10:50,027 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:10:50,027 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:10:50,027 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:10:50,027 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:10:50,027 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:10:50,027 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:10:50,027 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:10:50,027 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:10:50,027 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2023-12-08T16:10:50,027 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2023-12-08T16:10:50,135 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:10:50,135 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:10:50,136 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:10:50,136 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:10:50,194 [ERROR] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T16:10:50,194 [ERROR] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T16:11:03,038 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:11:03,038 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:11:09,682 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2311
2023-12-08T16:11:09,691 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:11:09,762 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:11:09,771 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2311
2023-12-08T16:11:09,772 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:11:09,772 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:11:09,772 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:11:09,772 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:11:09,772 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:11:09,781 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:11:09,791 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:11:09,791 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:11:09,791 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:11:09,791 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:11:09,791 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:11:09,791 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:11:09,791 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:11:09,811 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:11:09,811 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:11:09,811 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:11:09,811 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:11:09,812 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:11:09,812 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:11:09,812 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:11:09,812 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:11:09,812 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2023-12-08T16:11:09,812 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2023-12-08T16:11:09,910 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:11:09,910 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:11:09,911 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:11:09,911 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:11:30,825 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:11:30,825 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:11:37,620 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2319
2023-12-08T16:11:37,630 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:11:37,710 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:11:37,720 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2319
2023-12-08T16:11:37,730 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:11:37,730 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:11:37,730 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:11:37,730 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:11:37,730 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:11:37,739 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:11:37,749 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:11:37,749 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:11:37,750 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:11:37,750 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:11:37,750 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:367) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:194) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:11:37,750 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:367) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:36) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:194) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:11:37,759 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:11:37,759 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:11:37,759 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:11:37,759 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:11:37,760 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:11:37,760 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:11:37,760 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:11:37,778 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mymodel,model_version:default|#hostname:hiring-4,timestamp:1702051897
2023-12-08T16:11:37,760 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:11:37,807 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2023-12-08T16:11:37,807 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2023-12-08T16:11:37,887 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:11:37,887 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:11:37,896 [ERROR] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T16:11:37,896 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:11:37,896 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:11:37,896 [ERROR] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T16:12:11,820 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:12:11,820 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:12:18,599 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2327
2023-12-08T16:12:18,608 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:12:18,688 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:12:18,697 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2327
2023-12-08T16:12:18,697 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:12:18,698 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:12:18,703 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:12:18,703 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:12:18,703 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:12:18,703 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:12:18,707 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:12:18,708 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:12:18,708 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:12:18,708 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:12:18,708 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:12:18,708 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:12:18,708 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:12:18,709 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:12:18,709 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:12:18,709 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:12:18,709 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:12:18,709 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:12:18,709 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:12:18,709 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:12:18,709 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:12:18,710 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2023-12-08T16:12:18,710 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2023-12-08T16:12:18,814 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:12:18,814 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:12:18,817 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:12:18,817 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:13:13,714 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:13:13,714 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:13:20,420 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2334
2023-12-08T16:13:20,438 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:13:20,506 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:13:20,507 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2334
2023-12-08T16:13:20,507 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:13:20,515 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:13:20,515 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:13:20,515 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:13:20,515 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:13:20,516 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:13:20,516 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:13:20,516 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:13:20,516 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:13:20,516 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:13:20,516 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:13:20,516 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:13:20,517 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:13:20,517 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:13:20,517 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:13:20,517 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:13:20,517 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:13:20,517 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:13:20,525 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:13:20,525 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:13:20,535 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2023-12-08T16:13:20,535 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2023-12-08T16:13:20,577 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T16:13:20,577 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T16:13:20,585 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:13:20,586 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:13:20,585 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:13:20,586 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:14:49,556 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:14:49,556 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:14:56,341 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2342
2023-12-08T16:14:56,350 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:14:56,460 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:14:56,461 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2342
2023-12-08T16:14:56,461 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:14:56,461 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:14:56,461 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:14:56,461 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:14:56,470 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:14:56,470 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:14:56,470 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:14:56,472 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:14:56,472 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:14:56,491 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:14:56,491 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:14:56,492 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:14:56,492 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:14:56,502 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:14:56,502 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:14:56,502 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:14:56,502 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:14:56,510 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:14:56,510 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:14:56,547 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:14:56,547 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:14:56,624 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:14:56,624 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:14:56,619 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:14:56,619 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:17:56,999 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-12-08T16:17:56,999 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-12-08T16:17:57,028 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-12-08T16:17:57,028 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-12-08T16:17:57,478 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
2023-12-08T16:17:57,478 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
2023-12-08T16:17:58,113 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.9.0
TS Home: /usr/local/lib/python3.8/site-packages
Current directory: /source_dev/inference_server
Temp directory: /tmp
Metrics config path: /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 10240 M
Python executable: /usr/local/bin/python
Config file: /src/app/inference_server/torch.conf
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /model/model_dev/pytorch_model
Initial Models: mymodel=dolly-v2-3b.mar
Log dir: /source_dev/inference_server/logs
Metrics dir: /source_dev/inference_server/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /model/model_dev/pytorch_model
Model config: N/A
2023-12-08T16:17:58,113 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.9.0
TS Home: /usr/local/lib/python3.8/site-packages
Current directory: /source_dev/inference_server
Temp directory: /tmp
Metrics config path: /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 4
Max heap size: 10240 M
Python executable: /usr/local/bin/python
Config file: /src/app/inference_server/torch.conf
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /model/model_dev/pytorch_model
Initial Models: mymodel=dolly-v2-3b.mar
Log dir: /source_dev/inference_server/logs
Metrics dir: /source_dev/inference_server/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /model/model_dev/pytorch_model
Model config: N/A
2023-12-08T16:17:58,171 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-12-08T16:17:58,171 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-12-08T16:17:58,333 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: dolly-v2-3b.mar
2023-12-08T16:17:58,333 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: dolly-v2-3b.mar
2023-12-08T16:26:52,936 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mymodel
2023-12-08T16:26:52,936 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model mymodel
2023-12-08T16:26:52,944 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mymodel
2023-12-08T16:26:52,944 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model mymodel
2023-12-08T16:26:52,945 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mymodel loaded.
2023-12-08T16:26:52,945 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model mymodel loaded.
2023-12-08T16:26:52,946 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mymodel, count: 1
2023-12-08T16:26:52,946 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: mymodel, count: 1
2023-12-08T16:26:53,121 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-12-08T16:26:53,121 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-12-08T16:26:53,132 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:26:53,132 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:26:54,009 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-12-08T16:26:54,009 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-12-08T16:26:54,009 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-12-08T16:26:54,009 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-12-08T16:26:54,028 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-12-08T16:26:54,028 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-12-08T16:26:54,029 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-12-08T16:26:54,029 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-12-08T16:26:54,047 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-12-08T16:26:54,047 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-12-08T16:26:55,817 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-12-08T16:26:55,817 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-12-08T16:26:56,923 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702052816
2023-12-08T16:26:56,932 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:5.950298309326172|#Level:Host|#hostname:hiring-4,timestamp:1702052816
2023-12-08T16:26:56,932 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.758167266845703|#Level:Host|#hostname:hiring-4,timestamp:1702052816
2023-12-08T16:26:56,933 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:80.0|#Level:Host|#hostname:hiring-4,timestamp:1702052816
2023-12-08T16:26:56,933 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12389.75|#Level:Host|#hostname:hiring-4,timestamp:1702052816
2023-12-08T16:26:56,962 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3190.37109375|#Level:Host|#hostname:hiring-4,timestamp:1702052816
2023-12-08T16:26:56,963 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:22.2|#Level:Host|#hostname:hiring-4,timestamp:1702052816
2023-12-08T16:27:01,339 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mymodel,model_version:default|#hostname:hiring-4,timestamp:1702052821
2023-12-08T16:27:02,259 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2407
2023-12-08T16:27:02,288 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:27:02,386 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:27:02,395 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2407
2023-12-08T16:27:02,398 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change null -> WORKER_STARTED
2023-12-08T16:27:02,398 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change null -> WORKER_STARTED
2023-12-08T16:27:02,396 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:27:02,405 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:27:02,418 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:27:02,418 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:27:02,482 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:27:02,495 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1702052822495
2023-12-08T16:27:02,495 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1702052822495
2023-12-08T16:27:02,772 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - model_name: mymodel, batchSize: 1
2023-12-08T16:27:06,115 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Your torch version is 1.13.1+cu117 which does not support torch.compile
2023-12-08T16:27:06,116 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2023-12-08T16:27:06,116 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2023-12-08T16:27:06,601 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Backend worker process died.
2023-12-08T16:27:06,601 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2023-12-08T16:27:06,602 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_service_worker.py", line 258, in <module>
2023-12-08T16:27:06,610 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     worker.run_server()
2023-12-08T16:27:06,610 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_service_worker.py", line 226, in run_server
2023-12-08T16:27:06,610 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2023-12-08T16:27:06,610 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_service_worker.py", line 189, in handle_connection
2023-12-08T16:27:06,611 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2023-12-08T16:27:06,611 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_service_worker.py", line 131, in load_model
2023-12-08T16:27:06,611 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     service = model_loader.load(
2023-12-08T16:27:06,611 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/ts/model_loader.py", line 135, in load
2023-12-08T16:27:06,611 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2023-12-08T16:27:06,629 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/tmp/models/c4058286415d4a5fb7ab709bddb04323/llm_handler2.py", line 28, in initialize
2023-12-08T16:27:06,630 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     self.model = AutoModelForCausalLM.from_pretrained(
2023-12-08T16:27:06,630 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
2023-12-08T16:27:06,630 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     config, kwargs = AutoConfig.from_pretrained(
2023-12-08T16:27:06,631 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1048, in from_pretrained
2023-12-08T16:27:06,631 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
2023-12-08T16:27:06,631 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/configuration_utils.py", line 622, in get_config_dict
2023-12-08T16:27:06,631 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
2023-12-08T16:27:06,631 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/configuration_utils.py", line 677, in _get_config_dict
2023-12-08T16:27:06,639 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     resolved_config_file = cached_file(
2023-12-08T16:27:06,640 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -   File "/usr/local/lib/python3.8/site-packages/transformers/utils/hub.py", line 401, in cached_file
2023-12-08T16:27:06,648 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG -     raise EnvironmentError(
2023-12-08T16:27:06,659 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - OSError: /tmp/models/c4058286415d4a5fb7ab709bddb04323 does not appear to have a file named config.json. Checkout 'https://huggingface.co//tmp/models/c4058286415d4a5fb7ab709bddb04323/None' for available files.
2023-12-08T16:27:06,726 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:27:06,726 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:27:06,737 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:27:06,737 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:27:06,737 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:219) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:27:06,737 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:219) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:27:06,897 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mymodel, error: Worker died.
2023-12-08T16:27:06,897 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: mymodel, error: Worker died.
2023-12-08T16:27:06,915 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:27:06,915 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:27:06,916 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1702052826915
2023-12-08T16:27:06,916 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1702052826915
2023-12-08T16:27:06,916 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:27:06,916 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:27:06,916 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:27:06,916 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:27:06,917 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T16:27:06,917 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T16:27:07,074 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:27:07,074 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:27:07,077 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:27:07,077 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:27:07,926 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:27:07,926 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:27:14,806 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2425
2023-12-08T16:27:14,815 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:27:14,904 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:27:14,914 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2425
2023-12-08T16:27:14,914 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:27:14,914 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:27:14,914 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:27:14,914 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:27:14,934 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:27:14,934 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:27:14,935 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:27:14,935 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:27:14,944 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:27:14,944 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:27:14,964 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:27:14,964 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:27:14,965 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:27:14,965 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:27:14,915 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:27:14,994 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:27:14,994 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:27:14,995 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:27:14,995 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:27:14,998 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:27:14,998 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:27:14,998 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:27:15,005 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T16:27:15,005 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-12-08T16:27:15,084 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:27:15,084 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:27:16,005 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:27:16,005 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:27:22,875 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2433
2023-12-08T16:27:22,885 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:27:22,972 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:27:22,973 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2433
2023-12-08T16:27:22,973 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:27:22,973 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:27:22,974 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:27:22,974 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:27:22,982 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:27:22,984 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:27:22,984 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:27:22,984 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:27:22,984 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:27:22,984 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:27:22,984 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:27:22,985 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:27:22,985 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:27:22,985 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:27:22,985 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:27:22,985 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:27:22,985 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:27:22,985 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:27:22,982 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:27:23,001 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:27:23,001 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:27:22,985 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:27:23,005 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-12-08T16:27:23,005 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-12-08T16:27:23,101 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:27:23,101 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:27:23,181 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T16:27:23,181 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T16:27:25,007 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:27:25,007 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:27:31,795 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2441
2023-12-08T16:27:31,807 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:27:31,882 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:27:31,883 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2441
2023-12-08T16:27:31,883 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:27:31,883 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:27:31,885 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:27:31,885 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:27:31,885 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:27:31,885 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:27:31,902 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:27:31,905 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:27:31,905 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:27:31,913 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:27:31,913 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:27:31,913 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:27:31,913 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:27:31,914 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:27:31,914 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:27:31,914 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:27:31,914 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:27:31,914 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:27:31,914 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:27:31,914 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:27:31,914 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:27:31,914 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2023-12-08T16:27:31,914 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2023-12-08T16:27:32,018 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:27:32,018 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:27:32,024 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:27:32,024 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:27:34,917 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:27:34,917 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:27:41,703 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2449
2023-12-08T16:27:41,704 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:27:41,782 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:27:41,782 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2449
2023-12-08T16:27:41,783 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:27:41,783 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:27:41,790 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:27:41,790 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:27:41,791 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:27:41,791 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:27:41,802 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:27:41,811 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1702052861811
2023-12-08T16:27:41,802 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:27:41,811 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1702052861811
2023-12-08T16:27:41,812 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:27:41,812 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:27:41,812 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: DefaultChannelPromise@13ed838d(incomplete)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:243) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:131) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:403) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:119) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:209) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:27:41,812 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: DefaultChannelPromise@13ed838d(incomplete)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:243) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:131) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:403) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:119) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:209) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:27:41,821 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:27:41,891 [INFO ] W-9000-mymodel_1.0 ACCESS_LOG - /127.0.0.1:35814 "POST /predictions/mymodel HTTP/1.1" 500 40572
2023-12-08T16:27:41,910 [INFO ] W-9000-mymodel_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:hiring-4,timestamp:1702052861
2023-12-08T16:27:41,911 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.job.Job - Waiting time ns: 40425903399, Inference time ns: 40526107168
2023-12-08T16:27:41,911 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.job.Job - Waiting time ns: 40425903399, Inference time ns: 40526107168
2023-12-08T16:27:41,911 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:27:41,911 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:27:41,911 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:27:41,911 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:27:41,912 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:27:41,912 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:27:41,912 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:27:41,912 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:27:41,912 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2023-12-08T16:27:41,912 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.
2023-12-08T16:27:42,017 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:27:42,017 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:27:41,999 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:27:41,999 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:27:46,915 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:27:46,915 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:27:48,190 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mymodel,model_version:default|#hostname:hiring-4,timestamp:1702052868
2023-12-08T16:27:53,580 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2457
2023-12-08T16:27:53,589 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:27:53,659 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:27:53,668 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2457
2023-12-08T16:27:53,669 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:27:53,669 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:27:53,674 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:27:53,674 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:27:53,684 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:27:53,684 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:27:53,689 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:27:53,703 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:27:53,703 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:27:53,723 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:27:53,723 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:27:53,723 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:27:53,723 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:27:53,742 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:27:53,742 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:27:53,743 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:27:53,743 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:27:53,743 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:27:53,743 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:27:53,743 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:27:53,743 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:27:53,743 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2023-12-08T16:27:53,743 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.
2023-12-08T16:27:53,836 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:27:53,836 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:27:53,839 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:27:53,839 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:27:56,196 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:hiring-4,timestamp:1702052876
2023-12-08T16:27:56,197 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:5.950237274169922|#Level:Host|#hostname:hiring-4,timestamp:1702052876
2023-12-08T16:27:56,197 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:23.758228302001953|#Level:Host|#hostname:hiring-4,timestamp:1702052876
2023-12-08T16:27:56,216 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:80.0|#Level:Host|#hostname:hiring-4,timestamp:1702052876
2023-12-08T16:27:56,216 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12284.17578125|#Level:Host|#hostname:hiring-4,timestamp:1702052876
2023-12-08T16:27:56,217 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:3295.9140625|#Level:Host|#hostname:hiring-4,timestamp:1702052876
2023-12-08T16:27:56,217 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:22.9|#Level:Host|#hostname:hiring-4,timestamp:1702052876
2023-12-08T16:27:56,326 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1088, in emit
    stream.write(msg + self.terminator)
BrokenPipeError: [Errno 32] Broken pipe
Call stack:
  File "ts/metrics/metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "/usr/local/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('2457', 0)

2023-12-08T16:27:56,326 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/logging/__init__.py", line 1088, in emit
    stream.write(msg + self.terminator)
BrokenPipeError: [Errno 32] Broken pipe
Call stack:
  File "ts/metrics/metric_collector.py", line 29, in <module>
    check_process_mem_usage(sys.stdin)
  File "/usr/local/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py", line 40, in check_process_mem_usage
    logging.info("%s:%d", process, get_cpu_usage(process))
Message: '%s:%d'
Arguments: ('2457', 0)

2023-12-08T16:28:01,745 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:28:01,745 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:28:08,562 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2468
2023-12-08T16:28:08,581 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:28:08,650 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:28:08,661 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2468
2023-12-08T16:28:08,661 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:28:08,666 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:28:08,666 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:28:08,666 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:28:08,666 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:28:08,671 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:28:08,690 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:28:08,690 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:28:08,690 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:28:08,694 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:28:08,694 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:28:08,694 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:28:08,694 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:28:08,694 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:28:08,694 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:28:08,694 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:28:08,694 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:28:08,695 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:28:08,695 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:28:08,695 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:28:08,695 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:28:08,695 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2023-12-08T16:28:08,695 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.
2023-12-08T16:28:08,770 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:28:08,770 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:28:08,771 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:28:08,771 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:28:21,698 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:28:21,698 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:28:28,493 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2476
2023-12-08T16:28:28,502 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:28:28,580 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:28:28,591 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2476
2023-12-08T16:28:28,591 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:28:28,600 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:28:28,600 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:28:28,600 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:28:28,600 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:28:28,607 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:28:28,607 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:28:28,607 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:28:28,607 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:28:28,607 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:28:28,607 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:28:28,608 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:28:28,608 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:28:28,608 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:28:28,608 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:28:28,608 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:28:28,608 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:28:28,616 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:28:28,616 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:28:28,616 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2023-12-08T16:28:28,616 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.
2023-12-08T16:28:28,680 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:28:28,680 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:28:28,687 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:28:28,687 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:28:28,689 [ERROR] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T16:28:28,689 [ERROR] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - Unknown exception
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
2023-12-08T16:28:49,617 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:28:49,617 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:28:56,302 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2484
2023-12-08T16:28:56,322 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:28:56,408 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:28:56,410 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2484
2023-12-08T16:28:56,418 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:28:56,418 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:28:56,419 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:28:56,419 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:28:56,426 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:28:56,426 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1702052936426
2023-12-08T16:28:56,426 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:28:56,426 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1702052936426
2023-12-08T16:28:56,435 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:28:56,435 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:28:56,435 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: DefaultChannelPromise@7e8c0ce2(incomplete)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:243) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:131) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:403) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:119) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:209) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:28:56,435 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: DefaultChannelPromise@7e8c0ce2(incomplete)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:243) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:131) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.await(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at io.netty.util.concurrent.DefaultPromise.sync(DefaultPromise.java:403) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:119) ~[model-server.jar:?]
	at io.netty.channel.DefaultChannelPromise.sync(DefaultChannelPromise.java:30) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:209) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:28:56,436 [INFO ] W-9000-mymodel_1.0 ACCESS_LOG - /127.0.0.1:39312 "POST /predictions/mymodel HTTP/1.1" 500 68246
2023-12-08T16:28:56,436 [INFO ] W-9000-mymodel_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:hiring-4,timestamp:1702052936
2023-12-08T16:28:56,436 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.job.Job - Waiting time ns: 68225394426, Inference time ns: 68235797081
2023-12-08T16:28:56,436 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.job.Job - Waiting time ns: 68225394426, Inference time ns: 68235797081
2023-12-08T16:28:56,436 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:28:56,436 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:28:56,436 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:28:56,436 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:28:56,437 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:28:56,420 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:28:56,437 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:28:56,437 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:28:56,437 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:28:56,437 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:28:56,463 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:28:56,463 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:28:56,463 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2023-12-08T16:28:56,463 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.
2023-12-08T16:28:56,585 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:28:56,585 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:29:30,477 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:29:30,477 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:29:37,272 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2491
2023-12-08T16:29:37,273 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:29:37,353 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:29:37,353 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2491
2023-12-08T16:29:37,362 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:29:37,362 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:29:37,362 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:29:37,362 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:29:37,362 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:29:37,362 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:29:37,372 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:29:37,373 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:29:37,373 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:29:37,374 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:29:37,374 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:29:37,374 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:29:37,374 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:29:37,374 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:29:37,374 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:29:37,374 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:29:37,374 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:29:37,375 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:29:37,375 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:29:37,375 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:29:37,375 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:29:37,375 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2023-12-08T16:29:37,375 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.
2023-12-08T16:29:37,490 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:29:37,490 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:29:37,491 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:29:37,491 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:29:37,868 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:mymodel,model_version:default|#hostname:hiring-4,timestamp:1702052977
2023-12-08T16:30:32,401 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:30:32,401 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:30:39,211 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2499
2023-12-08T16:30:39,221 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:30:39,290 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:30:39,290 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2499
2023-12-08T16:30:39,291 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:30:39,294 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:30:39,291 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:30:39,294 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:30:39,303 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:30:39,303 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:30:39,304 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:30:39,304 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:30:39,304 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:30:39,304 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:30:39,304 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:30:39,304 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:30:39,315 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:30:39,315 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:30:39,315 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:30:39,323 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:30:39,323 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:30:39,324 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:30:39,324 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:30:39,324 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:30:39,324 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:30:39,324 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2023-12-08T16:30:39,324 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.
2023-12-08T16:30:39,427 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:30:39,427 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:30:39,427 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:30:39,427 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:32:08,345 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:32:08,345 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/local/bin/python, /usr/local/lib/python3.8/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml]
2023-12-08T16:32:15,166 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=2507
2023-12-08T16:32:15,175 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-12-08T16:32:15,253 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.8/site-packages/ts/configs/metrics.yaml.
2023-12-08T16:32:15,253 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - [PID]2507
2023-12-08T16:32:15,260 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Torch worker started.
2023-12-08T16:32:15,260 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Python runtime: 3.8.18
2023-12-08T16:32:15,261 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:32:15,261 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-12-08T16:32:15,262 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:32:15,262 [INFO ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-12-08T16:32:15,270 [INFO ] W-9000-mymodel_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-12-08T16:32:15,275 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:32:15,275 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-12-08T16:32:15,282 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:32:15,282 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-12-08T16:32:15,282 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:32:15,282 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081) ~[?:?]
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:276) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:424) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:191) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:840) [?:?]
2023-12-08T16:32:15,282 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:32:15,282 [DEBUG] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-mymodel_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-12-08T16:32:15,282 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:32:15,282 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-12-08T16:32:15,282 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:32:15,282 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stderr
2023-12-08T16:32:15,283 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:32:15,283 [WARN ] W-9000-mymodel_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-mymodel_1.0-stdout
2023-12-08T16:32:15,379 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:32:15,379 [INFO ] W-9000-mymodel_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stdout
2023-12-08T16:32:15,382 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
2023-12-08T16:32:15,382 [INFO ] W-9000-mymodel_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-mymodel_1.0-stderr
